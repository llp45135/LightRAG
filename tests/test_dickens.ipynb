{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dickens\n",
    "This notebook replicates the functionality of the `test_ollama.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.llm.siliconcloud import siliconcloud_embedding\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "import asyncio\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.openai import openai_complete_if_cache\n",
    "from lightrag.llm.siliconcloud import siliconcloud_embedding\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"./dickens\"\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "# 在导入部分添加\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 在代码的开头加载环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8b354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "        base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    return await siliconcloud_embedding(\n",
    "        texts,\n",
    "        model=\"netease-youdao/bce-embedding-base_v1\",\n",
    "        api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n",
    "        max_token_size=512,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3334181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function test\n",
    "async def test_funcs():\n",
    "    result = await llm_model_func(\"How are you?\")\n",
    "    print(\"llm_model_func: \", result)\n",
    "\n",
    "    result = await embedding_func([\"How are you?\"])\n",
    "    print(\"embedding_func: \", result)\n",
    "\n",
    "\n",
    "asyncio.run(test_funcs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=llm_model_func,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768, max_token_size=512, func=embedding_func\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model here\n",
    "model_choice = \"ollama\"  # Options: \"ollama\", \"openai\"\n",
    "\n",
    "llm_model_func = ollama_model_complete\n",
    "embedding_func = EmbeddingFunc(\n",
    "    embedding_dim=1024,\n",
    "    max_token_size=8192,\n",
    "    func=lambda texts: ollama_embed(\n",
    "        texts,\n",
    "        embed_model=\"bge-m3:latest\"\n",
    "    )\n",
    ")\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=llm_model_func, \n",
    "    llm_model_name='qwen2.5:7b-instruct-q4_K_M',\n",
    "    embedding_func=embedding_func,\n",
    "    llm_model_max_token_size=32768,\n",
    "    llm_model_kwargs={\"host\": \"http://localhost:11434\", \"options\": {\"num_ctx\": 32768}},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./book.txt\") as f:\n",
    "    rag.insert(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform naive search\n",
    "print(\"naive.....................\")\n",
    "print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform local search\n",
    "print(\"local.....................\")\n",
    "print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform global search\n",
    "print(\"global.....................\")\n",
    "print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hybrid search\n",
    "print(\"hybrid.....................\")\n",
    "print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mix search (Knowledge Graph + Vector Retrieval)\n",
    "print(\"mix.....................\")\n",
    "print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"mix\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
